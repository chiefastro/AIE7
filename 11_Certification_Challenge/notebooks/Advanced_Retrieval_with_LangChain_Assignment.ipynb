{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from uuid import uuid4\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = f\"AIM - Assignment 09 - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import getpass\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, the most common issues with loans appear to be related to mismanagement and errors by servicers, including:\\n\\n- Errors in loan balances and application of payments\\n- Incorrect or disputed information on credit reports\\n- Problems with how payments are being handled, such as the inability to pay down principal or paying extra funds\\n- Delays or lack of transparency when loans are transferred or sold to different servicers\\n- Discrepancies in interest rates and balances due to mishandling or improper adjustments\\n- Struggles with repayment plans, including incorrect or unjustified loan increases\\n- Unauthorized disclosures and privacy violations\\n- Issues with loan forgiveness, discharge, or cancellation processes\\n\\nOverall, a common theme is that borrowers face challenges due to errors, delays, lack of transparency, or misconduct by loan servicers and related agencies.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, some complaints were not handled in a timely manner. Specifically, the complaint filed with MOHELA on 03/28/25 was marked as \"No\" for response timeliness, indicating it was not addressed promptly. Additionally, multiple complaints related to delays or failure to resolve issues within expected timeframes are noted, such as the complaint from 04/14/25 regarding a complaint not being addressed for over 2-3 weeks and ongoing issues that have persisted for months or over a year.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans primarily because they faced financial hardships and complications related to loan management. The context highlights several reasons:\\n\\n1. **Accumulating interest and forbearance:** Many borrowers were only offered options like forbearance or deferment, during which interest continued to accrue, making repayment more difficult over time. Lowering monthly payments often resulted in interest surpassing payments, extending the repayment period and increasing total debt.\\n\\n2. **Lack of clear communication and misinformation:** Borrowers reported not being adequately informed about payment resumption dates, loan transfer details, or changes in their loan servicing. This lack of communication led to unintended delinquencies and negative credit impacts.\\n\\n3. **Inconsistent or confusing account management:** Discrepant loan balances, unrecognized transfer of loans between servicers without proper notification, and inability to access accurate account information contributed to repayment difficulties.\\n\\n4. **Difficulty in adjusting repayment plans:** Some borrowers found it impossible to make additional payments toward principal or pay off smaller loans quickly due to servicer restrictions, prolonging the debt.\\n\\n5. **Financial hardship and job instability:** Many borrowers took out loans assuming future employment prospects supported repayment but faced economic downturns, stagnant wages, or unemployment, rendering monthly payments unmanageable.\\n\\n6. **Mismanagement and lack of oversight:** Examples of long-term forbearance, mismanaged loans, and improper handling of debt (such as incorrect reporting to credit bureaus) caused additional obstacles to repayment.\\n\\nIn summary, unpaid loans often resulted from a combination of ongoing interest accumulation, poor communication, mismanagement by servicers, and borrowers' financial difficulties.\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, such as disputes over fees, difficulties applying payments correctly, receiving inaccurate information, or issues with loan terms and repayment processes. Several complaints mention frustration with how payments are handled, incorrect or misleading information from servicers, and issues that prevent proper repayment or understanding of loan details.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, all the complaints listed indicate that the company responded in a timely manner. Specifically, the complaints from 04/26/25, 04/01/25, 04/24/25, and 05/08/25 all have responses marked as \"Yes\" under the \"Timely response?\" field. Therefore, there is no evidence in this data to suggest that any complaints were not handled in a timely manner.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including:\\n\\n- Difficulty with payment plans and problems with their repayment options, such as being steered into wrong types of forbearances or not receiving proper assistance after applying for deferment or forbearance.\\n- Lack of communication or notification from the loan servicers regarding important information, such as changes in loan status, account updates, or approval of deferment/forbearance requests.\\n- Errors or issues with the loan servicing process, such as payments being reversed repeatedly, automatic payments being discontinued without notice, or billing problems.\\n- Loans being transferred between companies without proper notice, leading to unenrollment from autopay and subsequent missed or late payments.\\n- Disputes over the accuracy of billing and payments, which can result in accounts becoming overdue or in default.\\n- Some borrowers experienced increased loan balances due to capitalization of interest or other charges that they felt were unauthorized or unjustified.\\n\\nOverall, failures to pay back loans often stem from a combination of administrative errors, poor communication, and difficulties navigating the loan repayment process.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "Named entities - companies, addresses, people, etc.\n",
        "Embeddings of named entities can actually be misleading (imagine the embedding for Apple). Keyword search is a more direct way to retrieve data for named entities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, such as receiving inaccurate or bad information about the loan, errors in loan balances, misapplied payments, wrongful denials of payment plans, and mishandling of loan data. Many complaints also involve lack of proper communication, discrepancies in loan balances, and improper handling of personal information.\\n\\nTherefore, the most common issue is **problems with loan servicer misconduct, including errors in loan information, miscommunication, and mishandling of account details**.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, they indicate issues with timely handling. For example, one complaint from a consumer submitted over a year ago has not been resolved after nearly 18 months, and another complaint highlights a problem with customer service that has persisted for over 2-3 weeks. \\n\\nWhile the company responses state that some complaints were \"Closed with explanation\" and responses were \"Yes\" in terms of timeliness, the ongoing unresolved issues and long delays suggest that some complaints did not get handled in a fully timely manner from the consumer\\'s perspective.\\n\\nTherefore, yes, there are complaints that did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for several reasons, including:\\n\\n1. Lack of awareness and understanding: Borrowers were not aware that they needed to repay their loans or were not properly informed about repayment obligations, interest accumulation, and other loan details.\\n\\n2. Poor communication from lenders/servicers: Borrowers reported not receiving adequate notifications about payment due dates, loan transfers, or changes in loan status, which led to missed payments and confusion.\\n\\n3. Issues with account management: Problems such as being locked out of online accounts, incorrect information on credit reports, and mismatched account balances contributed to difficulties in managing repayment.\\n\\n4. Accumulation of interest and economic hardship: While options like forbearance or deferment were available, interest continued to accrue during such periods, increasing the total debt owed and making repayment more difficult.\\n\\n5. Insufficient income or financial hardship: Many borrowers found that their income levels or employment circumstances made it challenging to afford payments or increase payments to pay off loans faster.\\n\\n6. Misinformation and unrealistic expectations: Borrowers felt misled about their repayment obligations and the ease of paying off their loans, especially given the complexities of interest, wages, and potential loan forgiveness programs.\\n\\nOverall, a combination of lack of clear information, communication issues, financial hardship, and the complex nature of loan interest contributed to the failure to repay loans.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be problems related to mishandling by loan servicers, including errors in loan balances, misapplied payments, inaccurate information, and issues with loan payment plans and forgiveness or discharge processes. Many complaints highlight improper information about interest accrual, account mismanagement, lack of proper communication, and difficulties in correcting errors.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, there were complaints that did not get handled in a timely manner. Several complaints explicitly state that responses from the companies or the CFPB took longer than the expected timeframes, such as over 15 days or several months, and some complaints mention that the companies failed to respond at all despite multiple follow-ups. For example, complaints with responses marked as \"No\" for timely response or noting delays of over a year indicate that these issues were not addressed promptly.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans primarily due to issues such as mismanagement by loan servicers, mistaken or inaccurate information about their loan status, lack of proper communication or notification about payments, errors in account reporting, and inability to access or update their information. Additionally, some borrowers were steered into forbearance or other payment plans that accumulated interest and increased their debt, and others experienced systemic failures like transfers of their loans without proper notice, which led to missed payments or negative credit reporting. These factors created confusion, hindered borrowers' ability to stay current on their loans, and in some cases caused their debt to balloon beyond their initial borrowing amount.\""
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "Imagine you have 10 golden chunks in the vector DB. Each version of the user query may have an 80% probability of retrieving all the golden chunks, so each might pull 8 golden chunks. With n versions you have a 1 - (0.2)^n probability of getting all golden chunks (that probability is equal to the probability across a test set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be related to problems with loan servicing, such as errors in loan balances, misapplied payments, wrongful denials of payment plans, and misconduct by loan servicers. Many complaints highlight issues like incorrect information on credit reports, unfair increases in interest rates, and disputes over debt legitimacy, indicating that servicing problems are a prevalent concern.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, several complaints indicated that they did not receive responses in a timely manner. Specifically:\\n\\n- One complaint submitted on 03/28/25 by a consumer regarding federal student loan servicing was marked as \"Timely response?\": \"No,\" and the consumer reported that they had not heard back after multiple calls, despite the complaint indicating a 15-day response window. This suggests the complaint was not handled in a timely manner.\\n- Another complaint submitted on 04/11/25 also was marked as \"Timely response?\": \"No,\" with the consumer explicitly stating that they had not received any response despite waiting several weeks.\\n  \\nIn contrast, the complaint from 04/11/25 regarding a dispute settlement to credit bureaus was marked as \"Yes\" for timely response.\\n\\n**Conclusion:** Yes, at least some complaints did not get handled in a timely manner, specifically those from 03/28/25 and 04/11/25 regarding loan servicing issues.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to factors such as experiencing severe financial hardship, lack of proper information about repayment obligations, mismanagement by loan servicers, and issues related to the credibility and transparency of the educational institutions related to their loans. For example, some borrowers faced difficulties because their schools closed or misrepresented the value of their degrees, making it hard for them to find employment and generate income to repay their loans. Others experienced problems with loan servicing, such as not being properly notified about repayment details, being subjected to unverified debt collection practices, or encountering administrative errors like incorrect reporting or unauthorized account activity. Additionally, some borrowers relied on deferment and forbearance options, which increased their debt due to accumulating interest, further complicating repayment efforts.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be mismanagement and errors by loan servicers. This includes inaccurate loan balances, misapplied payments, wrongful denials of repayment plans, incorrect reporting to credit bureaus, improper handling of deferments or forbearances, and inadequate communication or notifications about account changes. Many complaints also involve wrongful transfers of loans without proper notice, improper classification of loan types, and errors in interest calculations, all of which cause financial hardship and stress for borrowers.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints data, yes, there are multiple complaints indicating that complaints did not get handled in a timely manner. Specifically:\\n\\n- One complaint from row 441 (received on 03/28/25) regarding a student loan issue was marked as \"Timely response?\": No, meaning it was not handled in a timely manner.\\n- Several other complaints, for example rows 400, 418, and 523, also indicate delays or failures to respond promptly, with some explicitly mentioning that responses took over the expected time or that issues remain unresolved despite repeated follow-up.\\n\\nTherefore, the answer is: Yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of systemic issues and mismanagement by loan servicers, including:\\n\\n1. **Lack of Clear Communication and Notification:** Many borrowers were not adequately informed about when their repayment was due or when their loans were transferred to different servicers, leading to missed or late payments. For example, some borrowers received no notification of loan status changes or repayment resumption, resulting in delinquency reports that damaged their credit scores.\\n\\n2. **Mismanagement and Errors by Servicers:** Errors such as incorrect account balances, misapplied payments, and improper reporting to credit bureaus contributed to borrowers falling behind. Several complaints mention incorrect delinquency reporting, failure to follow regulatory guidelines on notices, and improper account handling.\\n\\n3. **Unaffordable Payment Options & Interest Accumulation:** Borrowers often found themselves unable to afford increased payments or did not qualify for income-driven plans or loan forgiveness programs. As a result, interest continued to accrue, sometimes capitalizing and increasing the total debt significantly, making it impossible for some to pay off their loans.\\n\\n4. **Misleading or Insufficient Information:** Borrowers reported being steered into long-term forbearances or consolidations without being fully informed of the consequences, such as interest capitalization or the loss of forgiveness eligibility. This misguidance exacerbated their repayment difficulties.\\n\\n5. **Economic Hardship and Unforeseen Circumstances:** Many borrowers experienced life events such as unemployment, medical issues, or economic recessions, which reduced their ability to make payments. When combined with poor servicer communication and complex repayment options, repayment became unmanageable.\\n\\nIn summary, the failure to pay back loans was often due to a mix of inadequate communication, administrative errors, interest accumulation, misleading guidance, and genuine financial hardships—all compounded by systemic issues within the student loan servicing system.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be problems related to the mismanagement and mishandling of student loans by servicers and lenders. Specific frequent issues include:\\n\\n- Struggling to repay or problems with repayment plans and payment amounts.\\n- Discrepancies and errors in reported account status, including default notices and delinquency reports.\\n- Difficulties with loan account management, such as trouble accessing account information, switching servicers, or inaccurate reporting.\\n- Issues with improper or illegal use of reporting and collection practices.\\n- Problems with documentation, verification, and processing of eligibility for forgiveness or discharge.\\n- Lack of transparency, communication, and accountability from loan servicers.\\n\\nAmong these, a particularly recurring theme is **errors and disputes over account status, repayment amounts, and account handling**, which indicates that administrative errors and mismanagement are among the most frequent issues with loans in this data set.'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, all the responses from the companies were marked as \"Closed with explanation.\" Additionally, the details indicate that responses were submitted in a timely manner (\"Yes\" for timely response). Despite the timely responses, the complaints highlight ongoing issues such as lack of response to specific inquiries, unresolved disputes, or continued violations by the companies.\\n\\nTherefore, yes, some complaints did not get fully handled in a timely manner to the complainants\\' satisfaction, or were only partially addressed as the complaints persisted with unresolved issues.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People may fail to pay back their loans for various reasons, including issues with communication and transparency with lenders or servicers, difficulties in proving or verifying their loan status, problems with reporting errors or disputes about the legitimacy of the debt, and challenges related to payment processing or changes in payment plans. Additionally, some borrowers face complications due to administrative delays, bad information, or legal issues surrounding their loans, which can hinder repayment efforts.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "Semantic chunking might group these repetivite sentences together if they are semantically similar.\n",
        "\n",
        "Not sure if this is a thing, but to improve this I would consider just having an LLM chunk a document directly. Ask it to repeat back the document as a list/array where the elements are chunks, and the LLM decides how to keep semantically similar content contained within chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "acea2e4f00b5406886b7fae92b77e20b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db5f55f993574e54adb41a97d5ae73b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node 87554e14-a1cb-4fb7-9dce-45f70d9d372d does not have a summary. Skipping filtering.\n",
            "Node 186eb49d-de2e-4a69-a855-0aa3a2ab57cb does not have a summary. Skipping filtering.\n",
            "Node 7f77e5b2-8cbd-458c-bd27-7e67eb88b7f2 does not have a summary. Skipping filtering.\n",
            "Node 525cbbd3-a9f8-43a1-a9e7-786d79348914 does not have a summary. Skipping filtering.\n",
            "Node 124c9b6b-1e7e-43c6-b535-c7492337898b does not have a summary. Skipping filtering.\n",
            "Node aa2396cb-a241-4434-930a-be0c7f95b952 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7bae8aac9d44091ade13b26fe6e25c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08ca3a2a10814fd1a5db39921313e062",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c552fec433d445cdaf5b5b3be0479008",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed4ffd9ca394439b81bc184a2d81fd78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ad47b683e494629bf59594090988c90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(loan_complaint_data[:20], testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "invocables = {\n",
        "    \"naive\": naive_retrieval_chain,\n",
        "    \"bm25\": bm25_retrieval_chain, \n",
        "    \"contextual_compression\": contextual_compression_retrieval_chain,\n",
        "    \"multi_query\": multi_query_retrieval_chain,\n",
        "    \"parent_document\": parent_document_retrieval_chain, \n",
        "    \"ensemble\": ensemble_retrieval_chain, \n",
        "    \"semantic\": semantic_retrieval_chain, \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig\n",
        "import copy\n",
        "\n",
        "def run_eval_ragas(invocable, invocable_name, dataset):\n",
        "\n",
        "  dataset_this = copy.deepcopy(dataset)\n",
        "\n",
        "  for test_row in dataset_this:\n",
        "    response = invocable.invoke({\"question\" : test_row.eval_sample.user_input}, {\"tags\": [invocable_name]})\n",
        "    test_row.eval_sample.response = response[\"response\"].content\n",
        "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "\n",
        "  evaluation_dataset = EvaluationDataset.from_pandas(dataset_this.to_pandas())\n",
        "  evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
        "\n",
        "  custom_run_config = RunConfig(timeout=720)\n",
        "  result = evaluate(\n",
        "      dataset=evaluation_dataset,\n",
        "      metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy()],\n",
        "      llm=evaluator_llm,\n",
        "      run_config=custom_run_config\n",
        "  )\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db6ca1097e4a41d786b96d877e9e99f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00340a628e8c448d89bffc713a47b21e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e166f64568284660ba1a67d431797923",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "687e376a74d940fb8613f8fe96e694a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[3]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[22]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[13]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[23]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[19]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[27]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[18]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[24]: APIConnectionError(Connection error.)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb9876537f28462c9000f88bd1899401",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[1]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[39]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[9]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[34]: APIConnectionError(Connection error.)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "400265e1f20945c8af2c0425311f3136",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[6]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[26]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[16]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[35]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[23]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[26]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[30]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[34]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[14]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[22]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[6]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[10]: AttributeError('StringIO' object has no attribute 'statements')\n",
            "Exception raised in Job[9]: AttributeError('StringIO' object has no attribute 'statements')\n",
            "Exception raised in Job[26]: APIConnectionError(Connection error.)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c161a76f79943ddafa4488ddc939092",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[22]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[38]: APIConnectionError(Connection error.)\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "results = []\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    futures = [\n",
        "        executor.submit(run_eval_ragas, invocable, invocable_name, dataset)\n",
        "        for invocable_name, invocable in invocables.items()\n",
        "    ]\n",
        "    results = [f.result() for f in futures]\n",
        "\n",
        "results_d = {invocable_name: result for invocable_name, result in zip(invocables.keys(), results)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>factual_correctness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>experiment_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>naive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.522727</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.941829</td>\n",
              "      <td>naive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.987586</td>\n",
              "      <td>naive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.960614</td>\n",
              "      <td>naive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.944441</td>\n",
              "      <td>naive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.878788</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.951291</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.825000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.934657</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.923703</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    context_recall  faithfulness  factual_correctness  answer_relevancy  \\\n",
              "0              1.0      0.555556                 0.18          0.000000   \n",
              "1              1.0      0.522727                 0.15          0.941829   \n",
              "2              1.0      0.666667                 0.50          0.987586   \n",
              "3              0.0      0.000000                 0.24          0.960614   \n",
              "4              1.0      0.941176                 0.71          0.944441   \n",
              "..             ...           ...                  ...               ...   \n",
              "5              1.0      0.878788                 0.62          0.000000   \n",
              "6              NaN      0.880952                 0.88          0.951291   \n",
              "7              1.0      0.750000                 0.37          0.000000   \n",
              "8              1.0      0.825000                  NaN          0.934657   \n",
              "9              1.0      0.944444                 0.78          0.923703   \n",
              "\n",
              "   experiment_name  \n",
              "0            naive  \n",
              "1            naive  \n",
              "2            naive  \n",
              "3            naive  \n",
              "4            naive  \n",
              "..             ...  \n",
              "5         semantic  \n",
              "6         semantic  \n",
              "7         semantic  \n",
              "8         semantic  \n",
              "9         semantic  \n",
              "\n",
              "[70 rows x 5 columns]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "score_dfs = []\n",
        "for name, result in results_d.items():\n",
        "    score_df = pd.DataFrame(result.scores)\n",
        "    score_df['experiment_name'] = name\n",
        "    score_dfs.append(score_df)\n",
        "\n",
        "score_df = pd.concat(score_dfs)\n",
        "score_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>factual_correctness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>experiment_name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bm25</th>\n",
              "      <td>0.633333</td>\n",
              "      <td>0.640254</td>\n",
              "      <td>0.562222</td>\n",
              "      <td>0.749486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>contextual_compression</th>\n",
              "      <td>0.683333</td>\n",
              "      <td>0.768561</td>\n",
              "      <td>0.612857</td>\n",
              "      <td>0.842251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ensemble</th>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.858049</td>\n",
              "      <td>0.455000</td>\n",
              "      <td>0.665007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>multi_query</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.900093</td>\n",
              "      <td>0.616667</td>\n",
              "      <td>0.735416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>naive</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.731857</td>\n",
              "      <td>0.507500</td>\n",
              "      <td>0.824444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>parent_document</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.787431</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.742986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>semantic</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.777018</td>\n",
              "      <td>0.578571</td>\n",
              "      <td>0.663101</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        context_recall  faithfulness  factual_correctness  \\\n",
              "experiment_name                                                             \n",
              "bm25                          0.633333      0.640254             0.562222   \n",
              "contextual_compression        0.683333      0.768561             0.612857   \n",
              "ensemble                      0.966667      0.858049             0.455000   \n",
              "multi_query                   1.000000      0.900093             0.616667   \n",
              "naive                         0.833333      0.731857             0.507500   \n",
              "parent_document               0.833333      0.787431             0.475000   \n",
              "semantic                      1.000000      0.777018             0.578571   \n",
              "\n",
              "                        answer_relevancy  \n",
              "experiment_name                           \n",
              "bm25                            0.749486  \n",
              "contextual_compression          0.842251  \n",
              "ensemble                        0.665007  \n",
              "multi_query                     0.735416  \n",
              "naive                           0.824444  \n",
              "parent_document                 0.742986  \n",
              "semantic                        0.663101  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "score_aggs = score_df.groupby('experiment_name').mean()\n",
        "score_aggs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['naive', 'bm25', 'contextual_compression', 'multi_query', 'parent_document', 'ensemble', 'semantic'])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "tags = list(invocables.keys())\n",
        "by_tag = defaultdict(dict)\n",
        "\n",
        "for t in tqdm(tags):\n",
        "    by_tag[t] = client.get_run_stats(\n",
        "        project_names=[os.environ[\"LANGSMITH_PROJECT\"]],\n",
        "        # filter syntax lets you match an element in the tags array\n",
        "        filter=f\"has(tags, '{t}')\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run_count</th>\n",
              "      <th>latency_p50</th>\n",
              "      <th>latency_p99</th>\n",
              "      <th>first_token_p50</th>\n",
              "      <th>first_token_p99</th>\n",
              "      <th>total_tokens</th>\n",
              "      <th>prompt_tokens</th>\n",
              "      <th>completion_tokens</th>\n",
              "      <th>median_tokens</th>\n",
              "      <th>completion_tokens_p50</th>\n",
              "      <th>...</th>\n",
              "      <th>last_run_start_time</th>\n",
              "      <th>feedback_stats</th>\n",
              "      <th>run_facets</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>streaming_rate</th>\n",
              "      <th>total_cost</th>\n",
              "      <th>prompt_cost</th>\n",
              "      <th>completion_cost</th>\n",
              "      <th>cost_p50</th>\n",
              "      <th>cost_p99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>naive</th>\n",
              "      <td>140</td>\n",
              "      <td>0.097</td>\n",
              "      <td>5.97622</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>78448</td>\n",
              "      <td>75422</td>\n",
              "      <td>3026</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2025-07-29T18:07:50.680775</td>\n",
              "      <td>{}</td>\n",
              "      <td>[{'key': 'name', 'value': 'RunnableLambda', 'q...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008753</td>\n",
              "      <td>0.007542</td>\n",
              "      <td>0.00121</td>\n",
              "      <td>0.000856</td>\n",
              "      <td>0.00128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bm25</th>\n",
              "      <td>140</td>\n",
              "      <td>0.004</td>\n",
              "      <td>5.53122</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>44496</td>\n",
              "      <td>41779</td>\n",
              "      <td>2717</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2025-07-29T18:07:44.704082</td>\n",
              "      <td>{}</td>\n",
              "      <td>[{'key': 'name', 'value': 'RunnableLambda', 'q...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005265</td>\n",
              "      <td>0.004178</td>\n",
              "      <td>0.001087</td>\n",
              "      <td>0.000536</td>\n",
              "      <td>0.000795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>contextual_compression</th>\n",
              "      <td>150</td>\n",
              "      <td>0.2925</td>\n",
              "      <td>8.65653</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>29567</td>\n",
              "      <td>27045</td>\n",
              "      <td>2522</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2025-07-29T18:07:49.042365</td>\n",
              "      <td>{}</td>\n",
              "      <td>[{'key': 'name', 'value': 'RunnableLambda', 'q...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003713</td>\n",
              "      <td>0.002704</td>\n",
              "      <td>0.001009</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.000643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>multi_query</th>\n",
              "      <td>210</td>\n",
              "      <td>0.267</td>\n",
              "      <td>10.06614</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>109729</td>\n",
              "      <td>105427</td>\n",
              "      <td>4302</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2025-07-29T18:08:26.420350</td>\n",
              "      <td>{}</td>\n",
              "      <td>[{'key': 'name', 'value': 'RunnableLambda', 'q...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01163</td>\n",
              "      <td>0.009909</td>\n",
              "      <td>0.001721</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>0.001536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>parent_document</th>\n",
              "      <td>140</td>\n",
              "      <td>0.111</td>\n",
              "      <td>11.36261</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>43181</td>\n",
              "      <td>40126</td>\n",
              "      <td>3055</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2025-07-29T18:08:05.123571</td>\n",
              "      <td>{}</td>\n",
              "      <td>[{'key': 'name', 'value': 'RunnableLambda', 'q...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005235</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>0.001222</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>0.000912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ensemble</th>\n",
              "      <td>270</td>\n",
              "      <td>0.311</td>\n",
              "      <td>24.77662</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>161680</td>\n",
              "      <td>157113</td>\n",
              "      <td>4567</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2025-07-29T18:08:55.882714</td>\n",
              "      <td>{}</td>\n",
              "      <td>[{'key': 'name', 'value': 'VectorStoreRetrieve...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.017538</td>\n",
              "      <td>0.015711</td>\n",
              "      <td>0.001827</td>\n",
              "      <td>0.000487</td>\n",
              "      <td>0.002717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>semantic</th>\n",
              "      <td>140</td>\n",
              "      <td>0.089</td>\n",
              "      <td>9.86083</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>67636</td>\n",
              "      <td>64023</td>\n",
              "      <td>3613</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2025-07-29T18:07:50.507913</td>\n",
              "      <td>{}</td>\n",
              "      <td>[{'key': 'name', 'value': 'RunnableLambda', 'q...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007848</td>\n",
              "      <td>0.006402</td>\n",
              "      <td>0.001445</td>\n",
              "      <td>0.000821</td>\n",
              "      <td>0.000982</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       run_count latency_p50 latency_p99 first_token_p50  \\\n",
              "naive                        140       0.097     5.97622            None   \n",
              "bm25                         140       0.004     5.53122            None   \n",
              "contextual_compression       150      0.2925     8.65653            None   \n",
              "multi_query                  210       0.267    10.06614            None   \n",
              "parent_document              140       0.111    11.36261            None   \n",
              "ensemble                     270       0.311    24.77662            None   \n",
              "semantic                     140       0.089     9.86083            None   \n",
              "\n",
              "                       first_token_p99 total_tokens prompt_tokens  \\\n",
              "naive                             None        78448         75422   \n",
              "bm25                              None        44496         41779   \n",
              "contextual_compression            None        29567         27045   \n",
              "multi_query                       None       109729        105427   \n",
              "parent_document                   None        43181         40126   \n",
              "ensemble                          None       161680        157113   \n",
              "semantic                          None        67636         64023   \n",
              "\n",
              "                       completion_tokens median_tokens completion_tokens_p50  \\\n",
              "naive                               3026             0                     0   \n",
              "bm25                                2717             0                     0   \n",
              "contextual_compression              2522             0                     0   \n",
              "multi_query                         4302             0                     0   \n",
              "parent_document                     3055             0                     0   \n",
              "ensemble                            4567             0                     0   \n",
              "semantic                            3613             0                     0   \n",
              "\n",
              "                        ...         last_run_start_time feedback_stats  \\\n",
              "naive                   ...  2025-07-29T18:07:50.680775             {}   \n",
              "bm25                    ...  2025-07-29T18:07:44.704082             {}   \n",
              "contextual_compression  ...  2025-07-29T18:07:49.042365             {}   \n",
              "multi_query             ...  2025-07-29T18:08:26.420350             {}   \n",
              "parent_document         ...  2025-07-29T18:08:05.123571             {}   \n",
              "ensemble                ...  2025-07-29T18:08:55.882714             {}   \n",
              "semantic                ...  2025-07-29T18:07:50.507913             {}   \n",
              "\n",
              "                                                               run_facets  \\\n",
              "naive                   [{'key': 'name', 'value': 'RunnableLambda', 'q...   \n",
              "bm25                    [{'key': 'name', 'value': 'RunnableLambda', 'q...   \n",
              "contextual_compression  [{'key': 'name', 'value': 'RunnableLambda', 'q...   \n",
              "multi_query             [{'key': 'name', 'value': 'RunnableLambda', 'q...   \n",
              "parent_document         [{'key': 'name', 'value': 'RunnableLambda', 'q...   \n",
              "ensemble                [{'key': 'name', 'value': 'VectorStoreRetrieve...   \n",
              "semantic                [{'key': 'name', 'value': 'RunnableLambda', 'q...   \n",
              "\n",
              "                       error_rate streaming_rate total_cost prompt_cost  \\\n",
              "naive                         0.0            0.0   0.008753    0.007542   \n",
              "bm25                          0.0            0.0   0.005265    0.004178   \n",
              "contextual_compression        0.0            0.0   0.003713    0.002704   \n",
              "multi_query                   0.0            0.0    0.01163    0.009909   \n",
              "parent_document               0.0            0.0   0.005235    0.004013   \n",
              "ensemble                      0.0            0.0   0.017538    0.015711   \n",
              "semantic                      0.0            0.0   0.007848    0.006402   \n",
              "\n",
              "                       completion_cost  cost_p50  cost_p99  \n",
              "naive                          0.00121  0.000856   0.00128  \n",
              "bm25                          0.001087  0.000536  0.000795  \n",
              "contextual_compression        0.001009  0.000342  0.000643  \n",
              "multi_query                   0.001721  0.000288  0.001536  \n",
              "parent_document               0.001222  0.000406  0.000912  \n",
              "ensemble                      0.001827  0.000487  0.002717  \n",
              "semantic                      0.001445  0.000821  0.000982  \n",
              "\n",
              "[7 rows x 24 columns]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "langsmith_df = pd.DataFrame(by_tag).T\n",
        "langsmith_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>factual_correctness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>latency_p50</th>\n",
              "      <th>latency_p99</th>\n",
              "      <th>cost_p50</th>\n",
              "      <th>cost_p99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bm25</th>\n",
              "      <td>0.633333</td>\n",
              "      <td>0.640254</td>\n",
              "      <td>0.562222</td>\n",
              "      <td>0.749486</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>-5.53122</td>\n",
              "      <td>-0.000536</td>\n",
              "      <td>-0.000795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>contextual_compression</th>\n",
              "      <td>0.683333</td>\n",
              "      <td>0.768561</td>\n",
              "      <td>0.612857</td>\n",
              "      <td>0.842251</td>\n",
              "      <td>-0.2925</td>\n",
              "      <td>-8.65653</td>\n",
              "      <td>-0.000342</td>\n",
              "      <td>-0.000643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ensemble</th>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.858049</td>\n",
              "      <td>0.455000</td>\n",
              "      <td>0.665007</td>\n",
              "      <td>-0.311</td>\n",
              "      <td>-24.77662</td>\n",
              "      <td>-0.000487</td>\n",
              "      <td>-0.002717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>multi_query</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.900093</td>\n",
              "      <td>0.616667</td>\n",
              "      <td>0.735416</td>\n",
              "      <td>-0.267</td>\n",
              "      <td>-10.06614</td>\n",
              "      <td>-0.000288</td>\n",
              "      <td>-0.001536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>naive</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.731857</td>\n",
              "      <td>0.507500</td>\n",
              "      <td>0.824444</td>\n",
              "      <td>-0.097</td>\n",
              "      <td>-5.97622</td>\n",
              "      <td>-0.000856</td>\n",
              "      <td>-0.00128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>parent_document</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.787431</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.742986</td>\n",
              "      <td>-0.111</td>\n",
              "      <td>-11.36261</td>\n",
              "      <td>-0.000406</td>\n",
              "      <td>-0.000912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>semantic</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.777018</td>\n",
              "      <td>0.578571</td>\n",
              "      <td>0.663101</td>\n",
              "      <td>-0.089</td>\n",
              "      <td>-9.86083</td>\n",
              "      <td>-0.000821</td>\n",
              "      <td>-0.000982</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        context_recall  faithfulness  factual_correctness  \\\n",
              "bm25                          0.633333      0.640254             0.562222   \n",
              "contextual_compression        0.683333      0.768561             0.612857   \n",
              "ensemble                      0.966667      0.858049             0.455000   \n",
              "multi_query                   1.000000      0.900093             0.616667   \n",
              "naive                         0.833333      0.731857             0.507500   \n",
              "parent_document               0.833333      0.787431             0.475000   \n",
              "semantic                      1.000000      0.777018             0.578571   \n",
              "\n",
              "                        answer_relevancy latency_p50 latency_p99  cost_p50  \\\n",
              "bm25                            0.749486      -0.004    -5.53122 -0.000536   \n",
              "contextual_compression          0.842251     -0.2925    -8.65653 -0.000342   \n",
              "ensemble                        0.665007      -0.311   -24.77662 -0.000487   \n",
              "multi_query                     0.735416      -0.267   -10.06614 -0.000288   \n",
              "naive                           0.824444      -0.097    -5.97622 -0.000856   \n",
              "parent_document                 0.742986      -0.111   -11.36261 -0.000406   \n",
              "semantic                        0.663101      -0.089    -9.86083 -0.000821   \n",
              "\n",
              "                        cost_p99  \n",
              "bm25                   -0.000795  \n",
              "contextual_compression -0.000643  \n",
              "ensemble               -0.002717  \n",
              "multi_query            -0.001536  \n",
              "naive                   -0.00128  \n",
              "parent_document        -0.000912  \n",
              "semantic               -0.000982  "
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "score_aggs_combined = pd.concat([score_aggs, -langsmith_df[['latency_p50', 'latency_p99', 'cost_p50', 'cost_p99']]], axis=1)\n",
        "score_aggs_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>factual_correctness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>latency_p50</th>\n",
              "      <th>latency_p99</th>\n",
              "      <th>cost_p50</th>\n",
              "      <th>cost_p99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bm25</th>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>contextual_compression</th>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ensemble</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>multi_query</th>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>naive</th>\n",
              "      <td>4.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>parent_document</th>\n",
              "      <td>4.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>semantic</th>\n",
              "      <td>1.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        context_recall  faithfulness  factual_correctness  \\\n",
              "bm25                               7.0           7.0                  4.0   \n",
              "contextual_compression             6.0           5.0                  2.0   \n",
              "ensemble                           3.0           2.0                  7.0   \n",
              "multi_query                        1.5           1.0                  1.0   \n",
              "naive                              4.5           6.0                  5.0   \n",
              "parent_document                    4.5           3.0                  6.0   \n",
              "semantic                           1.5           4.0                  3.0   \n",
              "\n",
              "                        answer_relevancy  latency_p50  latency_p99  cost_p50  \\\n",
              "bm25                                 3.0          1.0          1.0       5.0   \n",
              "contextual_compression               1.0          6.0          3.0       2.0   \n",
              "ensemble                             6.0          7.0          7.0       4.0   \n",
              "multi_query                          5.0          5.0          5.0       1.0   \n",
              "naive                                2.0          3.0          2.0       7.0   \n",
              "parent_document                      4.0          4.0          6.0       3.0   \n",
              "semantic                             7.0          2.0          4.0       6.0   \n",
              "\n",
              "                        cost_p99  \n",
              "bm25                         2.0  \n",
              "contextual_compression       1.0  \n",
              "ensemble                     7.0  \n",
              "multi_query                  6.0  \n",
              "naive                        5.0  \n",
              "parent_document              3.0  \n",
              "semantic                     4.0  "
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "score_aggs_combined.rank(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "experiment_name\n",
              "multi_query               2.125\n",
              "contextual_compression    3.500\n",
              "semantic                  3.875\n",
              "naive                     4.375\n",
              "parent_document           4.375\n",
              "ensemble                  4.500\n",
              "bm25                      5.250\n",
              "dtype: float64"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# aggregated rankings for evals only\n",
        "score_aggs.rank(ascending=False).mean(axis=1).sort_values(ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "bm25                      2.25\n",
              "contextual_compression    3.00\n",
              "parent_document           4.00\n",
              "semantic                  4.00\n",
              "naive                     4.25\n",
              "multi_query               4.25\n",
              "ensemble                  6.25\n",
              "dtype: float64"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# aggregated rankings for cost/latency only\n",
        "(-langsmith_df[['latency_p50', 'latency_p99', 'cost_p50', 'cost_p99']]).rank(ascending=False).mean(axis=1).sort_values(ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "multi_query               3.1875\n",
              "contextual_compression    3.2500\n",
              "bm25                      3.7500\n",
              "semantic                  3.9375\n",
              "parent_document           4.1875\n",
              "naive                     4.3125\n",
              "ensemble                  5.3750\n",
              "dtype: float64"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# aggregated rankings for both evals and cost/latency\n",
        "score_aggs_combined.rank(ascending=False).mean(axis=1).sort_values(ascending=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n",
        "\n",
        "The winner of my experiments is `multi_query`, which provides the optimal balance of quality, latency, and cost. The cell below provides a complete picture of how each retriever ranks compared to all other retrievers on 8 different dimensions.\n",
        "\n",
        "The rankings largely meet expectations. For example, bm25 is fast but has the worst accuracy. The ensemble is slow and expensive, but did not produce enough accuracy boost to justify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>factual_correctness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>latency_p50</th>\n",
              "      <th>latency_p99</th>\n",
              "      <th>cost_p50</th>\n",
              "      <th>cost_p99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bm25</th>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>contextual_compression</th>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ensemble</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>multi_query</th>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>naive</th>\n",
              "      <td>4.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>parent_document</th>\n",
              "      <td>4.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>semantic</th>\n",
              "      <td>1.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        context_recall  faithfulness  factual_correctness  \\\n",
              "bm25                               7.0           7.0                  4.0   \n",
              "contextual_compression             6.0           5.0                  2.0   \n",
              "ensemble                           3.0           2.0                  7.0   \n",
              "multi_query                        1.5           1.0                  1.0   \n",
              "naive                              4.5           6.0                  5.0   \n",
              "parent_document                    4.5           3.0                  6.0   \n",
              "semantic                           1.5           4.0                  3.0   \n",
              "\n",
              "                        answer_relevancy  latency_p50  latency_p99  cost_p50  \\\n",
              "bm25                                 3.0          1.0          1.0       5.0   \n",
              "contextual_compression               1.0          6.0          3.0       2.0   \n",
              "ensemble                             6.0          7.0          7.0       4.0   \n",
              "multi_query                          5.0          5.0          5.0       1.0   \n",
              "naive                                2.0          3.0          2.0       7.0   \n",
              "parent_document                      4.0          4.0          6.0       3.0   \n",
              "semantic                             7.0          2.0          4.0       6.0   \n",
              "\n",
              "                        cost_p99  \n",
              "bm25                         2.0  \n",
              "contextual_compression       1.0  \n",
              "ensemble                     7.0  \n",
              "multi_query                  6.0  \n",
              "naive                        5.0  \n",
              "parent_document              3.0  \n",
              "semantic                     4.0  "
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "score_aggs_combined.rank(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
