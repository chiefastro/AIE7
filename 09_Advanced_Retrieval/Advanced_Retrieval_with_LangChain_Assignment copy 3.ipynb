{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import getpass\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issues with loans seem to involve mishandling by loan servicers, errors in loan balances and interest calculations, problems with repayment plans, miscommunication or lack of notification about account transfers, and incorrect or disputed information on credit reports. Many complaints also relate to the inability to apply payments correctly, unfair increases in interest, and mishandling of loan discharge or forgiveness.\\n\\nIn summary, the most frequent issue appears to be **dealing with errors and mismanagement by loan servicers, including inaccuracies in loan balances, interest, and repayment handling**.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, some complaints did not get handled in a timely manner. Specifically, at least one complaint from page 441 (Complaint ID: 12709087) was marked as \"Timely response?\": \"No,\" indicating it was not handled within the expected timeframe. The complaint involved delays in processing a graduated loan application and communication issues, with the individual reporting they had not heard back despite waiting several weeks.\\n\\nAdditionally, multiple other complaints mention extended periods of unresolved issues, such as complaints from pages 716 and 810, where borrowers reported waiting over a year or nearly 18 months without resolution or response.\\n\\nTherefore, the answer is: Yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans primarily due to a combination of factors highlighted in the complaints:\\n\\n1. **Miscommunication and Lack of Information:** Many borrowers were not adequately informed about when their repayment was to begin, the transfer of loan servicers without notification, or changes in their payment status. This lack of clarity led to unintentional delinquencies.\\n\\n2. **Compounding Interest and Unmanageable Payments:** Borrowers cited that interest continued to accrue even during forbearance or deferment periods, increasing overall debt. Lowering monthly payments often resulted in more interest accumulation, making it difficult to pay off the principal.\\n\\n3. **Financial Hardships and Economic Conditions:** Many borrowers experienced financial hardships, unemployment, or stagnant wages, which made their repayment plans unfeasible. For example, some relied on income-driven plans that were inaccessible or insufficient.\\n\\n4. **Issues with Loan Servicers:** Complaints mention difficulty applying payments correctly, servicer mismanagement, difficulty getting accurate information, and failure to offer flexible repayment options. Some borrowers reported that their payments were applied in a manner that mostly covered interest, prolonging debt.\\n\\n5. **Mismanagement and Unawareness:** Several individuals were unaware of their specific loan balances, interest rates, or when their payments resumed, leading to missed payments and credit damage.\\n\\n6. **Inadequate or No Response from Loan Servicers:** In many cases, loan servicers did not provide timely assistance, communication, or proper handling of inquiries, leaving borrowers uncertain about their repayment obligations.\\n\\nOverall, failure to repay was often less about irresponsibility and more about systemic issues—including poor communication, predatory repayment structures, and unexpected economic hardships—that hindered borrowers' ability to make consistent, manageable payments.\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, specifically issues such as incorrect or misleading information, difficulties in applying payments properly, and disputes over loan details or fees. Multiple complaints mention challenges like incorrect fee charges, trouble with payment application, and disputes about loan balances or information provided.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, all the complaints included in the context received timely responses from the companies involved. Specifically, the complaints regarding issues with the loan service and validation responses indicate that the companies responded within the required timeframe (\"Timely response?\": \"Yes\"). Therefore, no complaints in the given context appear to have gone unhandled or were delayed in handling.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including issues with the loan servicing process, miscommunication, and problems with payment plans. Specific factors highlighted in the complaints include:\\n\\n- Being steered into incorrect types of forbearances or payment plans, leading to increased principal and interest.\\n- Lack of communication from loan servicers regarding loan transfers, repayment status, or status updates, resulting in missed payments or misunderstandings.\\n- Payment reversals or technical issues with online payments, which were often blamed on the borrower’s bank even when payments were correctly made.\\n- Unclear or inadequate notification about changes in loan status, repayment requirements, or delinquency alerts, which led to borrowers being unaware of overdue payments.\\n- In some cases, servicing methods or bank automation issues caused payments to not be processed correctly, resulting in delinquency and damage to credit scores.\\n\\nOverall, failures to pay back loans often stem from systemic problems like poor communication from servicers, errors in payment processing, and inadequate assistance or guidance for borrowers.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "Named entities - companies, addresses, people, etc.\n",
        "Embeddings of named entities can actually be misleading (imagine the embedding for Apple). Keyword search is a more direct way to retrieve data for named entities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, including errors, miscommunications, and mishandling of information. Specific sub-issues mentioned include receiving bad information about the loan, incorrect account balances, lack of proper documentation, unauthorized transfers, privacy violations, and mishandling of data. These issues highlight that a frequent problem is the mishandling or miscommunication by loan servicers, which can lead to disputes, inaccuracies, and legal concerns.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, there are complaints that did not get handled in a timely manner. For example, the complaint about the student loan issues with Maximus Federal Services, Inc. has been open for over 18 months with no resolution, despite ongoing requests for review and response. Similarly, the complaint regarding unpaid payments with EdFinancial Services involves issues that have persisted for over 2-3 weeks or longer, with the customer still seeking resolution.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for several reasons, including:\\n\\n1. Lack of Awareness and Information: Borrowers often were not adequately informed about their obligation to repay loans or the details of their loans, such as interest accrual and repayment requirements.\\n2. Compounding Interest and Payment Options: The available options like forbearance or deferment allowed interest to continue accumulating, making the loans more difficult to pay off over time.\\n3. Unmanageable Payments: Borrowers faced difficulties in affording monthly payments due to financial hardships, stagnant wages, or economic circumstances, which prevented timely repayment.\\n4. Misleading or Inadequate Communication: Some borrowers were not notified properly about payment due dates, loan transfers, or the need to set up payment plans, leading to missed payments and reported late payments.\\n5. Loan Complexity and Growing Balances: Discrepancies in account information, unclear statements, and confusing loan balances contributed to borrowers’ inability to navigate repayment effectively.\\n6. Economic Factors: Economic challenges and financial hardship made it difficult for many to keep up with repayment schedules, even when they intended to do so.\\n\\nOverall, a combination of lack of clear information, financial hardship, and problematic loan servicing practices contributed to the failure of some borrowers to pay back their loans.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be problems related to mishandling and lack of transparency by loan servicers and agencies. This includes:\\n\\n- Errors in loan balances and interest calculations\\n- Misapplied payments and incorrect account information\\n- Unauthorized transfer or reassignment of loans without notice\\n- Bad or misleading information about loan terms, interest rates, and repayment options\\n- Breaches of privacy rights, including unauthorized access to personal data and potential FERPA violations\\n- Problems with loan forgiveness, discharge, or dispute process\\n- Issues with loan collection efforts, including silent calls and harassment\\n- Discrepancies and inaccuracies in reporting to credit bureaus\\n\\nOverall, many complaints highlight that borrowers experience frustration due to inadequate communication, errors, and questionable practices by loan management entities.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints data, yes, some complaints were not handled in a timely manner. Specifically:\\n\\n- Complaint ID 12973003 associated with EdFinancial Services was responded to promptly, and the response was marked as \"Timely response: Yes.\"\\n- Complaint ID 12709087 related to MOHELA was also acknowledged as responded to \"on time,\" with \"Timely response: Yes.\"\\n- However, Complaint ID 12654977 regarding MOHELA was marked as \"Timely response: No,\" indicating it was not handled in a timely manner.\\n- Complaint ID 13056764 involving EdFinancial Services was handled timely.\\n- Complaint ID 12975634 concerning Maximus (Aidvantage) was responded to \"on time.\"\\n- The complaint about Maximus (Aidvantage) with Complaint ID 13091395 was handled timely.\\n\\nIn the detailed complaint narratives, there are several instances where delays are evident, either due to the complaint being left unresolved for over a year or response times being explicitly marked as \"No\" for timeliness. For example, a complaint filed on XX/XX/XXXX related to a delay in resolution and ongoing issues suggests some complaints did not get addressed promptly.\\n\\nIn summary, while some complaints were handled promptly, at least one complaint (ID 12654977) was explicitly marked as not timely, indicating that not all complaints were addressed in a timely manner.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans mainly due to financial hardships, mismanagement, or lack of clear information. Many borrowers faced challenges such as accumulating interest that negated payments, inability to afford increased monthly payments, or loans being placed in forbearance for extended periods, which led to the continued growth of their debt. Some were misled about repayment options, interest accrual, or loan forgiveness programs, making repayment seem unrealistic. Others encountered improper handling by servicers, such as errors in loan balances, inadequate communication, or wrongful reporting to credit bureaus, which further complicated their ability to repay. Overall, systemic issues, lack of transparency, and unforeseen economic difficulties contributed to borrowers' struggles to pay back their loans.\""
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "Imagine you have 10 golden chunks in the vector DB. Each version of the user query may have an 80% probability of retrieving all the golden chunks, so each might pull 8 golden chunks. With n versions you have a 1 - (0.2)^n probability of getting all golden chunks (that probability is equal to the probability across a test set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be related to mishandling by loan servicers, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and issues with loan reporting and legitimacy. Many complaints cite system errors, improper reporting, and misconduct by servicers as significant problems faced by borrowers.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, based on the provided complaints, several complaints were not handled in a timely manner. Specifically, complaints regarding the processing of student loan applications by MOHELA and issues with loan servicing by Aidvantage were both marked as \"No\" for timely response. Additionally, the complaint about dispute settlements sent to credit bureaus by Nelnet, Inc. was marked as \"Yes\" for timely response, indicating it was handled promptly, but the other complaints clearly indicate delays. Therefore, the answer is that some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for various reasons, including financial hardship due to misrepresentations about the value of their education and job prospects, lack of proper notification or communication from loan servicers, inability to secure employment in their field, and difficulties managing repayment obligations. Additionally, some faced issues related to administrative errors or mismanagement, such as incorrect reporting of payments, failure to notify them of payment requirements, or complications arising from institutional closures and the dissolution of the Department of Education's oversight.\""
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans, particularly student loans, appears to be dealing with your lender or servicer, including problems such as errors in loan balances, misapplied payments, wrongful denials of payment plans, bad information about loans, and issues related to loan transfer or servicing mishandling. There are also frequent complaints about discrepancies in account information, difficulties in communication, and challenges with loan repayment or forgiveness programs.\\n\\nTherefore, the most common issue with loans is **problems related to handling and servicing the loan, including errors, miscommunication, and mismanagement by lenders or servicers.**'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, there are multiple complaints indicating that certain complaints did not get handled in a timely manner. Specifically:\\n\\n- Complaint ID 12709087 (row 441) about a federal student loan application not being processed and the delay exceeding the company’s own communicated timeframes. The response was marked as \"No\" for timely response.\\n- Complaint ID 12935889 (row 418) about a student\\'s account reporting late payments without proper notice, which was also marked as \"No\" for timely response.\\n- Complaint ID 13062402 (row 66) regarding inaccurate credit report information, which was responded to as \"Yes\" for timeliness, but indicates delays in correcting information even after promises.\\n- Multiple complaints about unresolved issues, delays in dispute investigations, and failure to respond or correct issues within legally mandated timeframes. Many are marked as \"No\" for being handled timely.\\n\\nTherefore, the answer is: Yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for various reasons, including:\\n\\n- Lack of proper notification and communication from loan servicers about payment due dates, account status, or changes in servicing (e.g., reports of not being notified about start dates, transfer of loans, or payment obligations).\\n- Difficulty understanding or accessing their payment options, such as income-driven repayment plans, loan forgiveness, or deferment programs.\\n- Accumulation of interest during deferment or forbearance periods, which increased the total amount owed and made repayment seem unmanageable.\\n- Complications caused by administrative errors, misapplied payments, incorrect account information, or poor record-keeping that led to inaccurate reporting and credit score drops.\\n- Financial hardships, including unemployment, health issues, and other personal difficulties, making it hard to meet payment obligations.\\n- Confusion and lack of transparency regarding loan balances, interest calculations, and the status of payments.\\n- In some cases, borrowers were unaware of their loan status due to inadequate communication, leading to missed payments and negative credit impacts.\\n\\nOverall, systemic issues such as poor communication from servicers, lack of accessible information about repayment options, and difficulties managing accruing interest contributed to many people's inability to repay their loans.\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints data, the most common issue with loans appears to be \"Dealing with your lender or servicer,\" which includes sub-issues such as receiving bad information about your loan, trouble with how payments are being handled, and problems with payment plans. Many complaints revolve around miscommunication, inaccuracies in loan information, difficulty verifying or understanding loan status, and issues with repayment arrangements. Therefore, the most common issue seems to be problems arising from loan servicers or lenders not managing or communicating loan information properly.'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, it appears that several complaints were marked as \"Closed with explanation\" and all responses noted \"Yes\" for timely responses. This suggests that these complaints were addressed within the expected timeframe.\\n\\nSpecifically, the complaints with the following complaint IDs indicate timely handling:\\n- 13331376 (Nelnet, Inc., IN)\\n- 13207537 (Maximus Federal Services, Inc., WA)\\n- 13425612 (Maximus Federal Services, Inc., VA)\\n- 13281034 (EdFinancial Services, NY)\\n- 12962044 (Nelnet, Inc., NJ)\\n- 13179688 (Nelnet, Inc., IL)\\n- 13020950 (Nelnet, Inc., OH)\\n- 13347464 (MOHELA, IL)\\n\\nHowever, despite the responses being timely, the complaints detail significant issues and unresolved disputes. There is no clear evidence from these snippets indicating that complaints were **not** handled in a timely manner. All responses to complaints were marked \"Yes\" for timely response, which suggests that the complaints were addressed within the expected review period.\\n\\n**Therefore, based on the available information, there is no indication that any complaints were not handled in a timely manner.**'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for various reasons, including issues with communication and transparency from the loan servicers, administrative challenges, and disputes over the legitimacy or accuracy of their loan information. For example, some borrowers experienced lack of proper documentation or clarity about their loan status, which led to misunderstandings and missed payments. Others faced stalling or delays from lenders or servicers when attempting to resolve issues or provide required documentation, causing frustration and difficulty in repayment. Additionally, some borrowers reported that their loans were improperly reported as delinquent or in default due to administrative errors or unresolved disputes over loan legitimacy, which negatively impacted their credit and ability to repay.\\n\\nIf you're facing such challenges, it's often related to administrative hurdles, miscommunication, or legal disputes over the validity of the loans, rather than a simple inability to pay.\""
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "Semantic chunking might group these repetivite sentences together if they are semantically similar.\n",
        "\n",
        "Not sure if this is a thing, but to improve this I would consider just having an LLM chunk a document directly. Ask it to repeat back the document as a list/array where the elements are chunks, and the LLM decides how to keep semantically similar content contained within chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - Assignment 09 - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_community.document_loaders import DirectoryLoader\n",
        "# from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "\n",
        "# path = \"data/\"\n",
        "# loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "# docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "269"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'GPL Ghostscript 10.00.0', 'creator': 'wkhtmltopdf 0.12.6', 'creationdate': \"D:20250418120630Z00'00'\", 'source': 'data/Academic_Calenders_Cost_of_Attendance_and_Packaging.pdf', 'file_path': 'data/Academic_Calenders_Cost_of_Attendance_and_Packaging.pdf', 'total_pages': 57, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20250418120630Z00'00'\", 'trapped': '', 'modDate': \"D:20250418120630Z00'00'\", 'creationDate': \"D:20250418120630Z00'00'\", 'page': 0}, page_content='Volume 3\\nAcademic Calendars, Cost of Attendance, and\\nPackaging\\nIntroduction\\nThis volume of the Federal Student Aid (FSA) Handbook discusses the academic calendar, payment period, and\\ndisbursement requirements for awarding aid under the Title IV student financial aid programs, determining a student9s\\ncost of attendance, and packaging Title IV aid.\\nThroughout this volume of the Handbook, the words \"we,\" \"our,\" and \"us\" refer to the United States Department of\\nEducation (the Department). The word \"you\" refers to the primary audience of the Handbook, school financial aid\\nadministrators. In other volumes of the Handbook we use \"institution,\" \"school,\" and \"college\" interchangeably, unless a\\nmore specific meaning is provided. In this volume we consistently use the term \"school.\" <HEA= refers to the Higher\\nEducation Act of 1965, as amended. Title IV refers to the student financial aid programs authorized under Title IV of the\\nHEA.\\nWe appreciate any comments that you have on this volume as well as the other volumes of the FSA Handbook. We revise\\nthe text based on questions and feedback from the financial aid community, so please reach out to us about how to\\nimprove the Handbook through the <Contact Customer Support= feature in our Partner Connect9s Help Center clicking on\\n<FSA Handbook= under the Topic section.\\nChanges for 2025-2026\\nThere are no major changes in Volume 3 for 2025-2026. However, we have made minor additions and clarifications to\\nexisting guidance in a few areas, as noted below.\\nChapter 1\\nIn Appendix B, we have added guidance on determining enrollment intensity for Pell Grant recipients who are enrolled in\\nsubscription-based programs.\\nChapter 2\\nWe have revised the discussion of <Allowable Costs= to clarify that for Pell Grant recipients with an enrollment\\nintensity below 50%, the cost of attendance (COA) is still based on the full-year costs for a full-time student. In this\\nsame section, we have also removed text noting that the FAFSA Simplification Act made certain changes to COA\\ncomponents. These changes are no longer new and were previously incorporated into the 2023-2024 FSA Handbook.\\nUnder <Tuition and Fees,= we have added health insurance premiums that are charged to all students as an\\nadditional example of a cost that may be included in this COA component.\\nWe have revised the text under <Costs of Obtaining a License, Certification, or First Professional Credential= to clarify\\nthat the guidance in this section pertains to credentials that are required for a student to practice or participate in\\nthe occupation the program is preparing the student to enter, and have added bar exam fees as an example of one\\nof the types of costs that may be included in this COA component.\\nUnder <Periods of Non-Attendance,= we have added language to clarify that a period of non-attendance is a period\\nduring which the student is not enrolled and is not otherwise engaged in any activity that is a requirement of the\\nstudent9s program of study.\\nChapter 3\\nUnder <Packaging Aid for Certain Dependents of Deceased Servicemembers or Public Safety Officers (Pell Grant Special\\nRule),= we have removed the note explaining that the Pell Grant Special Rule replaced the Iraq and Afghanistan Service\\nGrant (IASG) and Children of Fallen Heroes (CFH) provisions. This change is no longer new for 2025-2026.')"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce2a6616663c4243ae15ad46ad337067",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26e1cb357e934aa3b82f3ff8ecfe5f27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node 9998f96c-59e2-45b8-983b-1fcfcb851675 does not have a summary. Skipping filtering.\n",
            "Node 90c52aaa-7ae9-4c54-aadb-b3fbfaaf9487 does not have a summary. Skipping filtering.\n",
            "Node 515cc904-301d-46b6-b419-459bcd4c01a8 does not have a summary. Skipping filtering.\n",
            "Node abf8933c-687a-4b9d-9a7c-dfbdfac93396 does not have a summary. Skipping filtering.\n",
            "Node e6a50468-09ce-4f86-b356-40bd69b2daca does not have a summary. Skipping filtering.\n",
            "Node 636f8eaf-b73f-47f8-aedd-5d37853e2e00 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1649093dd0514228be5bc0d7d2aa0a34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d651be246db4c8aa60332dcd74fb933",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd7a932aefe84e93b7fa3d7af6be08d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6bb7c90eb8a4124b1fc9af0a72579e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bccd2df33f2d4adb85aca25ddf79cc99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(loan_complaint_data[:20], testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langsmith import Client\n",
        "\n",
        "# client = Client()\n",
        "\n",
        "# dataset_name = f\"Loan Synthetic Data - Assignment 09 - RAGAS - {uuid4().hex[0:8]}\"\n",
        "\n",
        "# langsmith_dataset = client.create_dataset(\n",
        "#     dataset_name=dataset_name,\n",
        "#     description=\"Loan Synthetic Data - Assignment 09 - RAGAS\"\n",
        "# )\n",
        "\n",
        "# for idx, row in dataset.to_pandas().iterrows():\n",
        "#   client.create_example(\n",
        "#       inputs={\n",
        "#           \"question\": row[\"user_input\"]\n",
        "#       },\n",
        "#       outputs={\n",
        "#           \"answer\": row[\"reference\"],\n",
        "#           \"context\": row[\"reference_contexts\"]\n",
        "#       },\n",
        "#       dataset_id=langsmith_dataset.id\n",
        "#   )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "invocables = {\n",
        "    \"naive\": naive_retrieval_chain,\n",
        "    \"bm25\": bm25_retrieval_chain, \n",
        "    \"contextual_compression\": contextual_compression_retrieval_chain,\n",
        "    \"multi_query\": multi_query_retrieval_chain,\n",
        "    \"parent_document\": parent_document_retrieval_chain, \n",
        "    \"ensemble\": ensemble_retrieval_chain, \n",
        "    \"semantic\": semantic_retrieval_chain, \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig\n",
        "import copy\n",
        "\n",
        "def run_eval_ragas(invocable, dataset):\n",
        "\n",
        "  dataset_this = copy.deepcopy(dataset)\n",
        "\n",
        "  for test_row in dataset_this:\n",
        "    response = invocable.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "    test_row.eval_sample.response = response[\"response\"].content\n",
        "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "\n",
        "  evaluation_dataset = EvaluationDataset.from_pandas(dataset_this.to_pandas())\n",
        "  evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
        "\n",
        "  custom_run_config = RunConfig(timeout=720)\n",
        "  result = evaluate(\n",
        "      dataset=evaluation_dataset,\n",
        "      metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall()],\n",
        "      llm=evaluator_llm,\n",
        "      run_config=custom_run_config\n",
        "  )\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97111e5135094c69b9590b38051d098a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62ce116fac1c47b4bf1fdc14882f970d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[33]: APIConnectionError(Connection error.)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f01858c928642ea91dc7a28f5f75c05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[17]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[29]: APIConnectionError(Connection error.)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e86f2e6b85644a1b1e480d9e5b322b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[32]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[36]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[49]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[28]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[50]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[21]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[42]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[50]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[22]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[40]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[31]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[49]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[2]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[56]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[57]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[57]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[47]: APIConnectionError(Connection error.)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bee5aa6725e4c6cb6d2db4427c0c0bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa27764966454953880c487fea92d3c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[2]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[47]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[6]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[27]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[37]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[48]: APIConnectionError(Connection error.)\n",
            "Exception raised in Job[31]: InternalServerError(upstream connect error or disconnect/reset before headers. reset reason: connection timeout)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "726ff38287fb47a69d9dae21ff25f83c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[49]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
            "Exception raised in Job[54]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
            "Exception raised in Job[34]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
            "Exception raised in Job[54]: TimeoutError()\n",
            "Exception raised in Job[44]: TimeoutError()\n",
            "Exception raised in Job[19]: TimeoutError()\n",
            "Exception raised in Job[29]: TimeoutError()\n",
            "Exception raised in Job[34]: TimeoutError()\n",
            "Exception raised in Job[39]: TimeoutError()\n",
            "Exception raised in Job[44]: TimeoutError()\n",
            "Exception raised in Job[4]: TimeoutError()\n",
            "Exception raised in Job[29]: TimeoutError()\n",
            "Exception raised in Job[59]: TimeoutError()\n",
            "Exception raised in Job[44]: TimeoutError()\n",
            "Exception raised in Job[54]: TimeoutError()\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "results = []\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    futures = [\n",
        "        executor.submit(run_eval_ragas, invocable, dataset)\n",
        "        for invocable_name, invocable in invocables.items()\n",
        "    ]\n",
        "    results = [f.result() for f in futures]\n",
        "\n",
        "results_d = {invocable_name: result for invocable_name, result in zip(invocables.keys(), results)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>factual_correctness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>experiment_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.962242</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>naive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.954630</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>naive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>naive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>naive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.961018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>naive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.944241</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    context_recall  faithfulness  factual_correctness  answer_relevancy  \\\n",
              "0              0.0      1.000000                 0.53          0.962242   \n",
              "1              0.0      0.000000                 0.18          0.954630   \n",
              "2              0.0      0.500000                 0.17          0.000000   \n",
              "3              0.0      0.500000                  NaN          0.000000   \n",
              "4              0.0      0.000000                 0.24          0.961018   \n",
              "..             ...           ...                  ...               ...   \n",
              "7              0.0      0.000000                 0.75          0.000000   \n",
              "8              0.0      0.000000                 0.48          0.000000   \n",
              "9              0.0      0.058824                 0.55          0.944241   \n",
              "10             0.0      0.000000                 0.73          0.000000   \n",
              "11             0.0      1.000000                  NaN          0.000000   \n",
              "\n",
              "    context_entity_recall experiment_name  \n",
              "0                0.000000           naive  \n",
              "1                0.000000           naive  \n",
              "2                0.000000           naive  \n",
              "3                0.333333           naive  \n",
              "4                0.000000           naive  \n",
              "..                    ...             ...  \n",
              "7                0.000000        semantic  \n",
              "8                     NaN        semantic  \n",
              "9                0.000000        semantic  \n",
              "10               0.000000        semantic  \n",
              "11               0.000000        semantic  \n",
              "\n",
              "[84 rows x 6 columns]"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "score_dfs = []\n",
        "for name, result in results_d.items():\n",
        "    score_df = pd.DataFrame(result.scores)\n",
        "    score_df['experiment_name'] = name\n",
        "    score_dfs.append(score_df)\n",
        "\n",
        "score_df = pd.concat(score_dfs)\n",
        "score_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>factual_correctness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_entity_recall</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>experiment_name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bm25</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.354061</td>\n",
              "      <td>0.362500</td>\n",
              "      <td>0.596466</td>\n",
              "      <td>0.030303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>contextual_compression</th>\n",
              "      <td>0.020833</td>\n",
              "      <td>0.355664</td>\n",
              "      <td>0.457000</td>\n",
              "      <td>0.784635</td>\n",
              "      <td>0.038194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ensemble</th>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.449142</td>\n",
              "      <td>0.334545</td>\n",
              "      <td>0.436101</td>\n",
              "      <td>0.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>multi_query</th>\n",
              "      <td>0.104167</td>\n",
              "      <td>0.379238</td>\n",
              "      <td>0.372500</td>\n",
              "      <td>0.785341</td>\n",
              "      <td>0.047619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>naive</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.336740</td>\n",
              "      <td>0.380000</td>\n",
              "      <td>0.711633</td>\n",
              "      <td>0.037037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>parent_document</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.465657</td>\n",
              "      <td>0.314444</td>\n",
              "      <td>0.542614</td>\n",
              "      <td>0.030303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>semantic</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.233155</td>\n",
              "      <td>0.473636</td>\n",
              "      <td>0.173299</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        context_recall  faithfulness  factual_correctness  \\\n",
              "experiment_name                                                             \n",
              "bm25                          0.000000      0.354061             0.362500   \n",
              "contextual_compression        0.020833      0.355664             0.457000   \n",
              "ensemble                      0.166667      0.449142             0.334545   \n",
              "multi_query                   0.104167      0.379238             0.372500   \n",
              "naive                         0.000000      0.336740             0.380000   \n",
              "parent_document               0.000000      0.465657             0.314444   \n",
              "semantic                      0.000000      0.233155             0.473636   \n",
              "\n",
              "                        answer_relevancy  context_entity_recall  \n",
              "experiment_name                                                  \n",
              "bm25                            0.596466               0.030303  \n",
              "contextual_compression          0.784635               0.038194  \n",
              "ensemble                        0.436101               0.025000  \n",
              "multi_query                     0.785341               0.047619  \n",
              "naive                           0.711633               0.037037  \n",
              "parent_document                 0.542614               0.030303  \n",
              "semantic                        0.173299               0.000000  "
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "score_df.groupby('experiment_name').mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
